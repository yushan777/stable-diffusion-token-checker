{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Stable Diffusion Dreambooth Token Checker\n",
        "\n",
        "Inspired and adapted from scripts and ideas by 2kpr\n",
        "\n",
        "https://www.reddit.com/r/StableDiffusion/comments/zc65l4/rare_tokens_for_dreambooth_training_stable/\n",
        "\n",
        "The purpose of this notebook is to check what a given token / token+class pair might generate from the model you are planning to train on. It will also show a breakdown of how your token is tokenized.   \n",
        "\n",
        "A 3x3 grid of images will be generated prompted with a given token from a list.  You can also include class words so that each token will be paired with the class and also added to the list.\n",
        "\n",
        "You can add a series of token words (comma separated). \n",
        "\n",
        "Output will include a breakdown of your token - i.e. so that long, convoluted token you came up with might actually be broken up into subtokens, and having a very strong prior association in the model. \n",
        " \n",
        "\n"
      ],
      "metadata": {
        "id": "y7nGjMsHOZIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Install Requirements\n",
        "\n",
        "!pip install -q --upgrade diffusers[torch]\n",
        "!pip install -q xformers==0.0.16\n",
        "!pip install -q transformers\n",
        "!pip install -q triton\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "import os\n",
        "import shutil\n",
        "from os import path\n",
        "from os.path import exists\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline, AutoencoderKL, DDIMScheduler\n",
        "import xformers\n",
        "from IPython.display import clear_output \n",
        "\n",
        "# font\n",
        "if exists('inconsolata.regular.ttf')==False:\n",
        "  #!wget https://github.com/larsenwork/Gidole/raw/master/Resources/GidoleFont/Gidole-Regular.ttf\n",
        "  !wget 'https://www.1001fonts.com/download/font/inconsolata.regular.ttf'\n",
        "\n",
        "# create output dir\n",
        "OUTPUT_DIR = \"output\"\n",
        "if path.exists(OUTPUT_DIR)==False:\n",
        "    os.mkdir(OUTPUT_DIR)\n",
        "\n",
        "#MODEL_PATH = \"models/stable-diffusion-v1-5\" # local path\n",
        "#VAE_PATH = \"models/sd-vae-ft-mse\" # local path\n",
        "\n",
        "MODEL_PATH = \"runwayml/stable-diffusion-v1-5\"\n",
        "#MODEL_PATH = \"stabilityai/stable-diffusion-2-1\"\n",
        "VAE_PATH = \"stabilityai/sd-vae-ft-mse\"\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(f'{MODEL_PATH}', vae = AutoencoderKL.from_pretrained(f'{VAE_PATH}', torch_dtype = torch.float16), revision=\"fp16\", torch_dtype = torch.float16, safety_checker = None)\n",
        "\n",
        "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "# =========================================================================================\n",
        "# Copy the vocab.json from the cache to current dir\n",
        "# =========================================================================================\n",
        "# tokenizer/vocab.json files are normally saved in cache e.g.\n",
        "# /root/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/xxxxxxxxxxxxxx/tokenizer/vocab.json\n",
        "\n",
        "def find_tokenizer_vocab_files():\n",
        "  name = 'vocab.json' \n",
        "  init_path = '/root/.cache/'\n",
        "  result = []\n",
        "  for root, dirs, files in os.walk(init_path):\n",
        "    if name in files:\n",
        "      result.append(os.path.join(root, name))\n",
        "  return result\n",
        "\n",
        "result = find_tokenizer_vocab_files()\n",
        "\n",
        "for cached_vocab_path in result:\n",
        "  print('-> ' + cached_vocab_path)\n",
        "\n",
        "  #split the cached dir path\n",
        "  dirs_list = cached_vocab_path.split('/')\n",
        "  # get repo name without username\n",
        "  HF_repo_name = MODEL_PATH.split('/')\n",
        "\n",
        "  #build dir path to copy vocab to\n",
        "  copy_to_dir = 'dictionary/' + HF_repo_name[1] + '/' + dirs_list[8] + '/' + dirs_list[9]\n",
        "\n",
        "\n",
        "  \n",
        "  # create target dir if not exist\n",
        "  os.makedirs(os.path.dirname(copy_to_dir), exist_ok=True)\n",
        "  #copy\n",
        "  shutil.copy(cached_vocab_path, copy_to_dir)\n",
        "  \n",
        "  #Set var\n",
        "  Vocab_File_Path = copy_to_dir\n",
        "\n",
        "print(\"Finished installing requirements.\") \n",
        "\n",
        "# ======================================================================================\n",
        "# DEFINE FUNCTIONS \n",
        "# ======================================================================================\n",
        "\n",
        "#@title 2. Define Functions\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from transformers import CLIPTokenizer, CLIPModel\n",
        "import json\n",
        "\n",
        "split_positions = []\n",
        "image_width_height = 512\n",
        "grid_rows = 3\n",
        "grid_cols = 3\n",
        "grid_width = image_width_height * grid_cols\n",
        "myFont = ImageFont.truetype(\"inconsolata.regular.ttf\", size=35)\n",
        "\n",
        "\n",
        "# ===========================================================================\n",
        "def tokenize(token):\n",
        "\n",
        "  with open(Vocab_File_Path, \"r\", encoding='utf-8') as f:\n",
        "    vocab = json.load(f)\n",
        "    vocab = {v: k for k, v in vocab.items()}\n",
        "\n",
        "  tokenizer = CLIPTokenizer.from_pretrained(MODEL_PATH, subfolder=\"tokenizer\",revision=\"fp16\")\n",
        "\n",
        "  #inputs = tokenizer(sys.argv[1], padding=True)\n",
        "  inputs = tokenizer(f'{token}', padding=True)\n",
        "  value = sum(inputs[\"input_ids\"])\n",
        "  value_str = str(value)\n",
        "  token_data = f'Token: {token}' + \" : \"\n",
        "\n",
        "  return_list = []\n",
        "  margin = 256\n",
        "\n",
        "  # add current data string to list\n",
        "  return_list.append(token_data)\n",
        "  token_data = '' # reset\n",
        "\n",
        "  for x in range(len(inputs[\"input_ids\"])):\n",
        "    #print(vocab[inputs[\"input_ids\"][x]] + \" \" + str(inputs[\"input_ids\"][x]))\n",
        "      \n",
        "    # don't include <|startoftext|> 49406 or <|endoftext|> 49407\n",
        "    if inputs[\"input_ids\"][x] != 49406 and inputs[\"input_ids\"][x] != 49407:\n",
        "      # add next sub-token\n",
        "      token_data += '[' + vocab[inputs[\"input_ids\"][x]] + ']'\n",
        "      # check string length      \n",
        "      if (get_text_size(token_data, myFont)[0] + margin) >= grid_width:\n",
        "        return_list.append(token_data.strip())\n",
        "        token_data = '' #reset string\n",
        "      if x < len(inputs[\"input_ids\"]) - 2:\n",
        "        token_data += ' '\n",
        "    \n",
        "  # add last or only string\n",
        "  return_list.append(token_data.strip())\n",
        "\n",
        "  print(\">>>\")\n",
        "  print(return_list )\n",
        "\n",
        "  return return_list\n",
        "\n",
        "# ===========================================================================\n",
        "def generate_grid():\n",
        "\n",
        "  # read word liste from file\n",
        "  #with open(\"4tokens_short.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  #\ttokenList = [line.strip() for line in f.readlines()]\n",
        "\n",
        "  # =============================================================\n",
        "  print(\"start\")\n",
        "  \n",
        "  titlebar = 128\n",
        "\n",
        "  for token in token_list:\n",
        "    token_data_list = tokenize(token)\n",
        "    print(\"Token: \" + token)\n",
        "    #create new image for grid\n",
        "    grid = Image.new('RGB', size=(grid_cols*image_width_height, (grid_rows*image_width_height)+titlebar))\n",
        "    with torch.autocast(\"cuda\"), torch.inference_mode():\n",
        "      for i in range(9):\n",
        "        image = pipe(token, num_inference_steps=20, num_images_per_prompt=1).images[0]\n",
        "        grid.paste(image, box=(i%grid_cols*image_width_height, (i//grid_cols*512)+titlebar))\n",
        "\n",
        "    # Call draw Method to add 2D graphics in an image\n",
        "    draw = ImageDraw.Draw(grid)\n",
        "  \n",
        "    # Add Text to grid\n",
        "    text_height = get_text_size(token_data_list[0], myFont)[1]\n",
        "    x = 0\n",
        "    y = 0\n",
        "    for line in token_data_list:\n",
        "      draw.text((x, y), line, font=myFont, fill=(255, 255, 255))\n",
        "      y += text_height\n",
        "\n",
        "    #draw.text((2, 12), token_data, font=myFont, fill=(255, 255, 255))\n",
        "  \n",
        "    #print(get_text_size(token_data, myFont))\n",
        "\n",
        "    # since quality is not a priority we will save in jpg with some compression\n",
        "    # to keep filesizes small and faster loading. \n",
        "    #print(\"Saving grid...\" + OUTPUT_DIR + \"/\" + token + \".png\")\n",
        "    #grid.save(OUTPUT_DIR + \"/\" + token + \".png\")\n",
        "    filename = token\n",
        "    if token ==' ' or token == '':\n",
        "      filename = 'empty'    \n",
        "    grid.save(OUTPUT_DIR + \"/\" + filename + \".jpg\", optimize=True, quality=60)\n",
        "\n",
        "# ===========================================================================\n",
        "def get_text_size(text_string, font):\n",
        "  # https://stackoverflow.com/a/46220683/9263761\n",
        "  ascent, descent = font.getmetrics()\n",
        "  text_width = font.getmask(text_string).getbbox()[2]\n",
        "  text_height = font.getmask(text_string).getbbox()[3] + descent\n",
        "  #print(\"text length = \" + str(text_width) )\n",
        "  return (text_width, text_height)\n"
      ],
      "metadata": {
        "id": "ku2PLmLxMreH",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Token / Class Words\n",
        "#@markdown add your token words you would like to test (comma separated)\n",
        "TOKENS = \"sks, supercalifragilisticexpialidocious12344567\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "delimeter = ''\n",
        "if ',' in TOKENS:\n",
        "    delimeter = ','\n",
        "else:\n",
        "    delimeter = ' ' #this will also clear empty strings\n",
        "    \n",
        "# split and remove any whitespaces\n",
        "token_list = [x.strip() for x in TOKENS.split(delimeter)]\n",
        "\n",
        "#for t in token_list:\n",
        "# print(t)\n",
        "\n",
        "#@markdown ___ \n",
        "\n",
        "#@markdown (Optional) add class to the list?\n",
        "\n",
        "class_person = True #@param {type:\"boolean\"}\n",
        "class_man = False #@param {type:\"boolean\"}\n",
        "class_woman = False #@param {type:\"boolean\"}\n",
        "class_artstyle = False #@param {type:\"boolean\"}\n",
        "class_other = \"\" #@param {type:\"string\"}\n",
        "\n",
        "token_and_person_list = []\n",
        "token_and_man_list = []\n",
        "token_and_woman_list = []\n",
        "token_and_artstyle_list = []\n",
        "token_and_other_list = []\n",
        "# =============================================\n",
        "if class_person==True:\n",
        "  for t in token_list:\n",
        "    token_and_person_list.append(f'{t} person')\n",
        "\n",
        "# =============================================\n",
        "if class_man==True:\n",
        "  for t in token_list:\n",
        "    token_and_man_list.append(f'{t} man')\n",
        "\n",
        "# =============================================\n",
        "if class_woman==True:\n",
        "  for t in token_list:\n",
        "    token_and_woman_list.append(f'{t} woman')\n",
        "\n",
        "# =============================================\n",
        "if class_artstyle==True:\n",
        "  for t in token_list:\n",
        "    token_and_artstyle_list.append(f'{t} artstyle')\n",
        "\n",
        "# =============================================\n",
        "if len(class_other) > 0 :\n",
        "  for t in token_list:\n",
        "    token_and_other_list.append(f'{t} {class_other}')\n",
        "\n",
        "# add token+class to original list\n",
        "token_list.extend(token_and_person_list)\n",
        "token_list.extend(token_and_man_list)\n",
        "token_list.extend(token_and_woman_list)\n",
        "token_list.extend(token_and_artstyle_list)\n",
        "token_list.extend(token_and_other_list)\n",
        "\n",
        "\n",
        "for t in token_list:\n",
        "    print(t)\n",
        "#@markdown For every token in your list, the class word will be appended to that token and added to the list. \\\n",
        "#@markdown Ex. If you have `'wow, owo, sks'` in your list, and select `'person'` class, then the list would become:\n",
        "#@markdown * wow\n",
        "#@markdown * owo\n",
        "#@markdown * sks\n",
        "#@markdown * wow person\n",
        "#@markdown * owo person\n",
        "#@markdown * sks person\n",
        "# add class word variant to list?\n"
      ],
      "metadata": {
        "id": "rrQr2_5xD9DH",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3. Generate!\n",
        "generate_grid()\n",
        "\n",
        "print(\"grids saved in 'output' directory\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mglPWeABMk3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Delete images from dir /content/output\n",
        "import os, shutil\n",
        "\n",
        "dir = \"/content/output\"\n",
        "\n",
        "#!rm -rf \"$dir\"\n",
        "\n",
        "for filename in os.listdir(dir):\n",
        "    file_path = os.path.join(dir, filename)\n",
        "    try:\n",
        "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "            os.unlink(file_path)\n",
        "        elif os.path.isdir(file_path):\n",
        "            shutil.rmtree(file_path)\n",
        "    except Exception as e:\n",
        "        print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
        "        \n",
        "print(\"you may need to refresh the file manager view if the files is still visible after deleting.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "N3GKXWWQPwwj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}